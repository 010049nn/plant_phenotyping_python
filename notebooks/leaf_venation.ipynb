{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phytometrics/plant_phenotyping_python/blob/dev/notebooks/leaf_venation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX17f2UXr0jg",
        "outputId": "623af717-88ac-4cec-93c6-a132de11cb01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-08 01:03:14--  https://zenodo.org/records/8020856/files/MorphometricsGroup/iwamasa-2022-v1.0.0.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.22.33, 188.185.33.206, 188.185.10.78, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.22.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59997203 (57M) [application/octet-stream]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  57.22M  18.5MB/s    in 3.1s    \n",
            "\n",
            "2023-11-08 01:03:17 (18.5 MB/s) - ‘data.zip’ saved [59997203/59997203]\n",
            "\n",
            "Archive:  data.zip\n",
            "a8f0e94cf10e8e5dc207b1c8127ff4aad16bad3b\n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/.gitignore  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/Dockerfile  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/LICENSE  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/README.md  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/\n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/interm/\n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/interm/non-treated-dataset/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/interm/non-treated-dataset/Zelkova_serrata_0.jpg  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/interm/non-treated-dataset/vein_Zelkova_serrata_0.jpg  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/interm/skeleton-non-treated-dataset/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/interm/skeleton-non-treated-dataset/skeleton_Zelkova_serrata_0.png  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/\n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/network-non-treated-dataset/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/network-non-treated-dataset/netsimile_features.csv  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/network-non-treated-dataset/node-link/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/network-non-treated-dataset/node-link/Zelkova_serrata_0.txt  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/network-non-treated-dataset/node-position/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/network-non-treated-dataset/node-position/Zelkova_serrata_0.json  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/segment-non-treated-dataset/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/segment-non-treated-dataset/Zelkova_serrata_0.png  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/processed/segment-non-treated-dataset/vein_Zelkova_serrata_0.png  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/data/raw/\n",
            " extracting: MorphometricsGroup-iwamasa-2022-eea5c23/data/raw/.gitkeep  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/data/raw/good_specimen_index.csv  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/docker-compose.yml  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/models/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/models/gray_resnet18_unet.pth  \n",
            "   creating: MorphometricsGroup-iwamasa-2022-eea5c23/notebooks/\n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/notebooks/extract_network-features.ipynb  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/notebooks/extract_network.ipynb  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/notebooks/segment_non-treated-leaf.ipynb  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/notebooks/skeletonization.ipynb  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/poetry.lock  \n",
            "  inflating: MorphometricsGroup-iwamasa-2022-eea5c23/pyproject.toml  \n"
          ]
        }
      ],
      "source": [
        "!wget -O data.zip https://zenodo.org/records/8020856/files/MorphometricsGroup/iwamasa-2022-v1.0.0.zip?download=1\n",
        "!unzip data.zip\n",
        "!rm data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TazYPq-UsoZU",
        "outputId": "2a85ff22-5690-451d-9580-3803b26242fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting segmentation-models-pytorch==0.2.0\n",
            "  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch==0.2.0) (0.16.0+cu118)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch==0.2.0)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.6.3 (from segmentation-models-pytorch==0.2.0)\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.4.12 (from segmentation-models-pytorch==0.2.0)\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (2.1.0+cu118)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12404 sha256=9fea45a929a690931e2b7a24293c6ce373562c5f5c50420470a7a3e65bb98fdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/3a/b0/0b4c443c380bd934701b0a25e4aed76479e4fcaf1a6f955664\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=36f84e8b4ca8d3a63e2b93d9d43508814d5cc40af235f7d49d5899a5e7f0b34d\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.0 timm-0.4.12\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEHNqNl_szkJ"
      },
      "source": [
        "## 葉脈検出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J8EEFhmCr-Gk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as albu\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# class Dataset(Dataset):\n",
        "#     def __init__(\n",
        "#             self,\n",
        "#             data_dir=None,\n",
        "#             augmentation=None,\n",
        "#             preprocessing=None,):\n",
        "#         self.image_paths = [p for p in glob(str(data_dir / '*.jpg'))]\n",
        "#         self.mask_paths = [p.split('.jpg')[0] + '.npy' for p in self.image_paths]\n",
        "#         self.augmentation = augmentation\n",
        "#         self.preprocessing = preprocessing\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         image = cv2.imread(self.image_paths[idx])\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "#         mask = np.load(self.mask_paths[idx])\n",
        "#         mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "#         if self.augmentation:\n",
        "#             sample = self.augmentation(image=image, mask=mask)\n",
        "#             image, mask = sample['image'], sample['mask']\n",
        "\n",
        "#         if self.preprocessing:\n",
        "#             sample = self.preprocessing(image=image, mask=mask)\n",
        "#             image, mask = sample['image'], sample['mask']\n",
        "\n",
        "#         return image, mask\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing t      ransform\n",
        "\n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function\n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "\n",
        "    \"\"\"\n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)\n",
        "\n",
        "def detect_green_color(img):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    hsv_min = np.array([30, 64, 0])\n",
        "    hsv_max = np.array([90, 255, 255])\n",
        "    mask = cv2.inRange(hsv, hsv_min, hsv_max)\n",
        "    masked_img = cv2.bitwise_and(img, img, mask=mask)\n",
        "    return mask, masked_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQpEUGLnskvZ",
        "outputId": "e13280f9-f49e-4deb-e130-79bae6a19751"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BASE_DIR = '/content/MorphometricsGroup-iwamasa-2022-eea5c23'\n",
        "# INFERENCE_PATH = os.path.join(BASE_DIR, 'data/processed/segment-non-treated-dataset')\n",
        "# IMAGE_DIR = os.path.join(BASE_DIR,  'data/interm/non-treated-dataset')\n",
        "INFERENCE_PATH = '/content'\n",
        "\n",
        "ENCODER = 'resnet18'\n",
        "DECODER = 'unet'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "ACTIVATION = 'sigmoid'\n",
        "DEVICE = 'cuda'\n",
        "crop_size = 512\n",
        "buffer_size = 32\n",
        "patch_size = crop_size - 2 * buffer_size\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(\n",
        "    ENCODER,\n",
        "    ENCODER_WEIGHTS\n",
        ")\n",
        "preprocessing = get_preprocessing(preprocessing_fn)\n",
        "best_model = torch.load(\n",
        "    os.path.join(BASE_DIR , f'models/gray_{ENCODER}_{DECODER}.pth')\n",
        ")\n",
        "\n",
        "# image_paths = glob(os.path.join(IMAGE_DIR, '*'))\n",
        "count = 0\n",
        "\n",
        "# for i, path in tqdm(enumerate(image_paths), total=len(image_paths)):\n",
        "path = \"/content/MorphometricsGroup-iwamasa-2022-eea5c23/data/interm/non-treated-dataset/Zelkova_serrata_0.jpg\"\n",
        "file_name = \"Zelkova_serrata_0.jpg\"\n",
        "\n",
        "# detect grean-area\n",
        "im = cv2.imread(path)\n",
        "contours, hierarchy = cv2.findContours(\n",
        "    detect_green_color(im)[0],\n",
        "    cv2.RETR_TREE,\n",
        "    cv2.CHAIN_APPROX_SIMPLE\n",
        ")\n",
        "max_contours = max(contours, key=lambda x: cv2.contourArea(x))\n",
        "mask = cv2.drawContours(\n",
        "    np.zeros_like(im[:,:,0]),\n",
        "    [max_contours],\n",
        "    -1,\n",
        "    color=255,\n",
        "    thickness=-1\n",
        ")\n",
        "im = cv2.imread(path, 0)\n",
        "im = np.where(mask==255, 255-im, 255)\n",
        "\n",
        "im = np.expand_dims(im, axis=-1)\n",
        "im = np.repeat(im, 3, axis=-1)\n",
        "new_shape = [-(-im.shape[i]//patch_size)*patch_size + 2*buffer_size  for i in [0, 1]] + [3]\n",
        "reshape_im = np.ones(new_shape, dtype=np.uint8) * 255\n",
        "reshape_im[buffer_size:im.shape[0]+buffer_size, buffer_size:im.shape[1]+buffer_size, :] = im\n",
        "\n",
        "output_im = np.zeros((new_shape[0], new_shape[1]), dtype=np.float32)\n",
        "for h_i, h in enumerate(range(buffer_size, new_shape[0]-buffer_size, patch_size)):\n",
        "    for w_i, w in enumerate(range(buffer_size, new_shape[1]-buffer_size, patch_size)):\n",
        "        h -= buffer_size\n",
        "        w -= buffer_size\n",
        "        tmp_im = reshape_im[h:h+crop_size, w:w+crop_size, :]\n",
        "        if reshape_im.mean() > 0:\n",
        "            tmp_im = preprocessing(image=tmp_im)\n",
        "            x_tensor = torch.from_numpy(tmp_im['image']).to(DEVICE).unsqueeze(0)\n",
        "            tmp_output_im = best_model.predict(x_tensor).squeeze().cpu().numpy()\n",
        "            h += buffer_size\n",
        "            w += buffer_size\n",
        "            output_im[h:h+patch_size, w:w+patch_size] = tmp_output_im[buffer_size:-buffer_size, buffer_size:-buffer_size]\n",
        "output_im = ((output_im)*255).astype(np.uint8)\n",
        "cv2.imwrite(os.path.join(INFERENCE_PATH, (file_name+'.png')), im)\n",
        "cv2.imwrite(os.path.join(INFERENCE_PATH, ('vein_'+file_name+'.png')), output_im)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pdG_cExymH"
      },
      "source": [
        "## skeletonization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tUjL6vdiwS8D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "\n",
        "def get_skeletonizeimage(path, output_dir):\n",
        "    new_path = path.split('/')[-1][5:][:-4]\n",
        "    im = cv2.imread(path, 0)\n",
        "    im = skeletonize(im>100).astype(np.uint8) * 255\n",
        "    cv2.imwrite(os.path.join(output_dir, ('skeleton_'+new_path+'.png')), im)\n",
        "\n",
        "get_skeletonizeimage(\"/content/vein_Zelkova_serrata_0.jpg.png\", \"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sfiv0kKK0x4E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tqdm.auto import tqdm\n",
        "from collections import deque, defaultdict\n",
        "import joblib\n",
        "\n",
        "\n",
        "def get_nodes(im: np.array):\n",
        "    H, W = im.shape\n",
        "    im_nodes = np.zeros((H+2, W+2), dtype=int)\n",
        "    for dh in [0, 1, 2]:\n",
        "        for dw in [0, 1, 2]:\n",
        "            if dh == 1 and dw == 1:\n",
        "                im_nodes[dh:H+dh, dw:W+dw] += im * 100\n",
        "            else:\n",
        "                im_nodes[dh:H+dh, dw:W+dw] += im\n",
        "    im_nodes = (im_nodes[1:H+1, 1:W+1]>=103)*255 + (im_nodes[1:H+1, 1:W+1]==101)*255\n",
        "\n",
        "    return im_nodes\n",
        "\n",
        "\n",
        "def get_nodepoints(im_nodes: np.array):\n",
        "    H, W = im_nodes.shape\n",
        "    nodes = defaultdict(list)\n",
        "    _count = 0\n",
        "    check = np.zeros_like(im_nodes, dtype=int)\n",
        "    for h in range(H):\n",
        "        for w in range(W):\n",
        "            if check[h, w] == 1:\n",
        "                continue\n",
        "            check[h, w] = 1\n",
        "            if im_nodes[h, w] == 255:\n",
        "                nodes[_count] = [[h, w]]\n",
        "                que = deque([[h, w]])\n",
        "                while que:\n",
        "                    _h, _w = que.popleft()\n",
        "                    for dh in [-1, 0, 1]:\n",
        "                        for dw in [-1, 0, 1]:\n",
        "                            if dh == 0 and dw == 0:\n",
        "                                continue\n",
        "                            nh = _h + dh\n",
        "                            nw = _w + dw\n",
        "                            if 0<=nh<H and 0<=nw<W and check[nh, nw]==0 and im_nodes[nh, nw]==255:\n",
        "                                nodes[_count].append([nh, nw])\n",
        "                                check[nh, nw] = 1\n",
        "                                que.append([nh, nw])\n",
        "                _count += 1\n",
        "    return nodes\n",
        "\n",
        "\n",
        "def get_connectnodes(im: np.array, nodes: list):\n",
        "    H, W = im.shape\n",
        "    im_nodes_id = np.zeros_like(im, dtype=int) - 1\n",
        "    for _key in nodes.keys():\n",
        "        for h, w in nodes[_key]:\n",
        "            im_nodes_id[h, w] = _key\n",
        "\n",
        "    nodes_output = [[] for _ in range(len(nodes))]\n",
        "    for _key in nodes.keys():\n",
        "        que = deque(nodes[_key].copy())\n",
        "        check = nodes[_key].copy()\n",
        "        while que:\n",
        "            _h, _w = que.popleft()\n",
        "            for dh in [-1, 0, 1]:\n",
        "                for dw in [-1, 0, 1]:\n",
        "                    if dh == 0 and dw == 0: continue\n",
        "                    nh = _h + dh\n",
        "                    nw = _w + dw\n",
        "                    if 0<nh<H and 0<=nw<W and im[nh, nw]==1 and (not [nh, nw] in check):\n",
        "                        check.append([nh, nw])\n",
        "                        if im_nodes_id[nh, nw] != -1:\n",
        "                            nodes_output[_key].append(im_nodes_id[nh, nw])\n",
        "                        else:\n",
        "                            que.append([nh, nw])\n",
        "    return nodes_output\n",
        "\n",
        "\n",
        "def get_linkingnodes(connect_edges: defaultdict):\n",
        "    nodes_check = [0] * len(connect_edges)\n",
        "    nodes_size = [0] * len(connect_edges)\n",
        "\n",
        "    for i in range(len(connect_edges)):\n",
        "        if nodes_check[i] == 1: continue\n",
        "        nodes_check[i] = 1\n",
        "\n",
        "        que = deque([i])\n",
        "        path_list = [i]\n",
        "        while que:\n",
        "            x = que.popleft()\n",
        "            for y in connect_edges[x]:\n",
        "                if nodes_check[y] == 1: continue\n",
        "                nodes_check[y] = 1\n",
        "\n",
        "                que.append(y)\n",
        "                path_list.append(y)\n",
        "\n",
        "        for _path in path_list:\n",
        "            nodes_size[_path] = len(path_list)\n",
        "\n",
        "    _max = max(nodes_size)\n",
        "    _set = set([i for i in range(len(nodes_size)) if nodes_size[i] == _max])\n",
        "\n",
        "    nodes_output = [[] for _ in range(len(connect_edges))]\n",
        "    for x in range(len(connect_edges)):\n",
        "        if not x in _set: continue\n",
        "        for y in connect_edges[x]:\n",
        "            if not y in _set: continue\n",
        "            nodes_output[x].append(y)\n",
        "    return nodes_output, _set\n",
        "\n",
        "\n",
        "def get_network(path):\n",
        "    new_path = path.split('/')[-1][9:][:-4]\n",
        "\n",
        "    try:\n",
        "        im = cv2.imread(path, 0)\n",
        "        im = (im > 100).astype(int)\n",
        "        im_nodes = get_nodes(im)\n",
        "        nodes = get_nodepoints(im_nodes)\n",
        "        connect_edges = get_connectnodes(im, nodes)\n",
        "        connect_edges_linking, nodes_linking = get_linkingnodes(connect_edges)\n",
        "\n",
        "        # out_path = new_path + '.txt'\n",
        "        out_path = new_path + 'node-link.txt'\n",
        "        with open(os.path.join(LINKOUTPUT_DIR, out_path), 'wb') as f:\n",
        "            pickle.dump(connect_edges_linking, f)\n",
        "\n",
        "        # out_path = new_path + '.json'\n",
        "        out_path = new_path + 'node-position.json'\n",
        "        with open(os.path.join(POSOUTPUT_DIR, out_path), 'wb') as f:\n",
        "            pickle.dump(nodes, f)\n",
        "        return None\n",
        "\n",
        "    except:\n",
        "        out_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WMl_iArY007K"
      },
      "outputs": [],
      "source": [
        "# BASE_DIR = '../'\n",
        "# IMAGE_DIR = os.path.join(BASE_DIR, 'data/interm/skeleton-non-treated-dataset')\n",
        "# OUTPUT_DIR = os.path.join(BASE_DIR, 'data/processed/network-non-treated-dataset')\n",
        "# LINKOUTPUT_DIR = os.path.join(OUTPUT_DIR, 'node-link')\n",
        "# POSOUTPUT_DIR = os.path.join(OUTPUT_DIR, 'node-position')\n",
        "\n",
        "OUTPUT_DIR = \"./\"\n",
        "LINKOUTPUT_DIR = \"./\"\n",
        "POSOUTPUT_DIR = \"./\"\n",
        "\n",
        "# image_paths = glob(os.path.join(IMAGE_DIR, '*'))\n",
        "\n",
        "get_network(\"/content/skeleton_Zelkova_serrata_0.jpg.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si-TIsyJ34Y0"
      },
      "source": [
        "## calculate network features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "622KM01P1ydJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "\n",
        "import re\n",
        "import cv2\n",
        "import pickle\n",
        "from scipy.stats import skew, kurtosis\n",
        "import networkx as nx\n",
        "from networkx.generators.ego import ego_graph\n",
        "\n",
        "\n",
        "from collections import deque, defaultdict\n",
        "\n",
        "\n",
        "def get_netsimile(path):\n",
        "    with open(path, \"rb\") as f:\n",
        "        link = pickle.load(f)\n",
        "    features = []\n",
        "    G = nx.Graph()\n",
        "    for u, e in enumerate(link):\n",
        "        for v in e:\n",
        "            G.add_edge(u, v)\n",
        "\n",
        "    # degree\n",
        "    ids = [i for i, p in enumerate(link) if p]\n",
        "    degrees = {i: nx.degree(G)[i] for i in ids}\n",
        "\n",
        "    # clustering_coefficient\n",
        "    clustering = nx.clustering(G)\n",
        "\n",
        "    # neighbor(one-hop)\n",
        "    neighbor_degrees = {}\n",
        "    for u, e in enumerate(link):\n",
        "        if not e: continue\n",
        "        d = []\n",
        "        for v in e:\n",
        "            d.append(degrees[v])\n",
        "        neighbor_degrees[u] = sum(d) / len(d)\n",
        "\n",
        "    # clustering_neighbor(one-hop)\n",
        "    neighbor_clustering = {}\n",
        "    for u, e in enumerate(link):\n",
        "        if not e: continue\n",
        "        d = []\n",
        "        for v in e:\n",
        "            d.append(clustering[v])\n",
        "        neighbor_clustering[u] = sum(d) / len(d)\n",
        "\n",
        "    ego_in_degree = []\n",
        "    ego_out_degree = []\n",
        "    ego_neighbor_nodes = []\n",
        "\n",
        "    for u, e in enumerate(link):\n",
        "        if not e: continue\n",
        "        ego1 = ego_graph(G, u, radius=1)\n",
        "        ego2 = ego_graph(G, u, radius=2)\n",
        "\n",
        "        # edges_in_egonet(one-hop)\n",
        "        ego_in_degree.append(ego1.number_of_edges())\n",
        "\n",
        "        # edges_outgoing_from_egonet(one-hop)\n",
        "        ego_out_degree.append(sum(dict(ego1.degree()).values()) - ego1.number_of_edges())\n",
        "\n",
        "        # neighbor_of_egonet(one-hop)\n",
        "        ego_neighbor_nodes.append(ego2.number_of_nodes() - ego1.number_of_nodes())\n",
        "\n",
        "\n",
        "    output = pd.DataFrame({\n",
        "        'degree': degrees.values(),\n",
        "        'clustering_coefficient': clustering.values(),\n",
        "        'neighbor(one-hop)': neighbor_degrees.values(),\n",
        "        'clustering_neighbor(one-hop)': neighbor_clustering.values(),\n",
        "        'edges_in_egonet(one-hop)': ego_in_degree,\n",
        "        'edges_outgoing_from_egonet(one-hop)': ego_out_degree,\n",
        "        'neighbor_of_egonet(one-hop)': ego_neighbor_nodes\n",
        "    })\n",
        "\n",
        "    for col in output.columns:\n",
        "        values = output[col].values\n",
        "        features.append(np.mean(values))\n",
        "        features.append(np.std(values))\n",
        "        features.append(skew(values))\n",
        "        features.append(kurtosis(values))\n",
        "\n",
        "    return [path, features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "13c4526f091943648a222b40e3b86386",
            "67f3fd2a74284d33982b9efa64054d5e",
            "b0a28d93c21b4d50b4f8b1594725effe",
            "ccf3dc42d13846268101e0e9786dd5f9",
            "d822ed667b0c43479745404d14a693a5",
            "2a300978f2e0499fa4808306bd6d60ba",
            "f0be69cd3f944d0781a756f69cbc9427",
            "3f6e16236a974b6485f30c8c0e4d8e39",
            "0ae0d5c9e3234ee38a5ed7aac8f70154",
            "6724ac8434e8458d8d41155a8dd03501",
            "6f4afca7f59f45dd95612aab08be6315"
          ]
        },
        "id": "HituUFcF2oD_",
        "outputId": "547ffb02-f2f6-4957-9703-5babe9c05e96"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13c4526f091943648a222b40e3b86386",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "columns = [\n",
        "    'degree',\n",
        "    'clustering_coefficient',\n",
        "    'neighbor(one-hop)',\n",
        "    'clustering_neighbor(one-hop)',\n",
        "    'edges_in_egonet(one-hop)',\n",
        "    'edges_outgoing_from_egonet(one-hop)',\n",
        "    'neighbor_of_egonet(one-hop)'\n",
        "]\n",
        "\n",
        "statistics = [\n",
        "    '_average',\n",
        "    '_standard_deviation',\n",
        "    '_skewness',\n",
        "    '_kurtosis',\n",
        "]\n",
        "\n",
        "BASE_DIR = '../'\n",
        "# NETWORK_DIR = os.path.join(BASE_DIR, 'data/processed/network-non-treated-dataset')\n",
        "NETWORK_DIR = \"./\"\n",
        "# link_paths = glob(os.path.join(NETWORK_DIR, 'node-link/*'))\n",
        "link_paths = [\"/content/Zelkova_serrata_0.jpgnode-link.txt\"]\n",
        "# 単一ファイル解析になおしておく\n",
        "\n",
        "feature_cols = []\n",
        "for col in columns:\n",
        "    for sta in statistics:\n",
        "        feature_cols.append(col + sta)\n",
        "\n",
        "feature_results = joblib.Parallel(n_jobs=-1)(\n",
        "    joblib.delayed(get_netsimile)(path) for path in tqdm(link_paths)\n",
        ")\n",
        "\n",
        "paths = [l[0].split('/')[-1][:-4] for l in feature_results]\n",
        "features = [l[1] for l in feature_results]\n",
        "\n",
        "feature_df = pd.concat([\n",
        "    pd.DataFrame({'path': paths}),\n",
        "    pd.DataFrame(features, columns=feature_cols)\n",
        "], axis=1)\n",
        "feature_df.to_csv(os.path.join(NETWORK_DIR,  'netsimile_features.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "Z2UnAtHF48oD",
        "outputId": "bdb1feb1-fbfc-4c90-a367-bae384b274ae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3d422540-dde8-486a-b1e8-85a0bec39121\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>degree_average</th>\n",
              "      <th>degree_standard_deviation</th>\n",
              "      <th>degree_skewness</th>\n",
              "      <th>degree_kurtosis</th>\n",
              "      <th>clustering_coefficient_average</th>\n",
              "      <th>clustering_coefficient_standard_deviation</th>\n",
              "      <th>clustering_coefficient_skewness</th>\n",
              "      <th>clustering_coefficient_kurtosis</th>\n",
              "      <th>neighbor(one-hop)_average</th>\n",
              "      <th>...</th>\n",
              "      <th>edges_in_egonet(one-hop)_skewness</th>\n",
              "      <th>edges_in_egonet(one-hop)_kurtosis</th>\n",
              "      <th>edges_outgoing_from_egonet(one-hop)_average</th>\n",
              "      <th>edges_outgoing_from_egonet(one-hop)_standard_deviation</th>\n",
              "      <th>edges_outgoing_from_egonet(one-hop)_skewness</th>\n",
              "      <th>edges_outgoing_from_egonet(one-hop)_kurtosis</th>\n",
              "      <th>neighbor_of_egonet(one-hop)_average</th>\n",
              "      <th>neighbor_of_egonet(one-hop)_standard_deviation</th>\n",
              "      <th>neighbor_of_egonet(one-hop)_skewness</th>\n",
              "      <th>neighbor_of_egonet(one-hop)_kurtosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Zelkova_serrata_0.jpgnode-link</td>\n",
              "      <td>2.508331</td>\n",
              "      <td>0.933899</td>\n",
              "      <td>-0.69121</td>\n",
              "      <td>-0.793665</td>\n",
              "      <td>0.007724</td>\n",
              "      <td>0.051841</td>\n",
              "      <td>7.72542</td>\n",
              "      <td>73.453735</td>\n",
              "      <td>2.889092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.521707</td>\n",
              "      <td>-0.563398</td>\n",
              "      <td>2.534139</td>\n",
              "      <td>0.968582</td>\n",
              "      <td>-0.521707</td>\n",
              "      <td>-0.563398</td>\n",
              "      <td>4.551672</td>\n",
              "      <td>1.954041</td>\n",
              "      <td>-0.062643</td>\n",
              "      <td>-1.127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d422540-dde8-486a-b1e8-85a0bec39121')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d422540-dde8-486a-b1e8-85a0bec39121 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d422540-dde8-486a-b1e8-85a0bec39121');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                             path  degree_average  degree_standard_deviation  \\\n",
              "0  Zelkova_serrata_0.jpgnode-link        2.508331                   0.933899   \n",
              "\n",
              "   degree_skewness  degree_kurtosis  clustering_coefficient_average  \\\n",
              "0         -0.69121        -0.793665                        0.007724   \n",
              "\n",
              "   clustering_coefficient_standard_deviation  clustering_coefficient_skewness  \\\n",
              "0                                   0.051841                          7.72542   \n",
              "\n",
              "   clustering_coefficient_kurtosis  neighbor(one-hop)_average  ...  \\\n",
              "0                        73.453735                   2.889092  ...   \n",
              "\n",
              "   edges_in_egonet(one-hop)_skewness  edges_in_egonet(one-hop)_kurtosis  \\\n",
              "0                          -0.521707                          -0.563398   \n",
              "\n",
              "   edges_outgoing_from_egonet(one-hop)_average  \\\n",
              "0                                     2.534139   \n",
              "\n",
              "   edges_outgoing_from_egonet(one-hop)_standard_deviation  \\\n",
              "0                                           0.968582        \n",
              "\n",
              "   edges_outgoing_from_egonet(one-hop)_skewness  \\\n",
              "0                                     -0.521707   \n",
              "\n",
              "   edges_outgoing_from_egonet(one-hop)_kurtosis  \\\n",
              "0                                     -0.563398   \n",
              "\n",
              "   neighbor_of_egonet(one-hop)_average  \\\n",
              "0                             4.551672   \n",
              "\n",
              "   neighbor_of_egonet(one-hop)_standard_deviation  \\\n",
              "0                                        1.954041   \n",
              "\n",
              "   neighbor_of_egonet(one-hop)_skewness  neighbor_of_egonet(one-hop)_kurtosis  \n",
              "0                             -0.062643                                -1.127  \n",
              "\n",
              "[1 rows x 29 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNL-ctA7dJfJ"
      },
      "source": [
        "## PCAとか"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrezV9LEdLTS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyObpnu/Rc+MixUFRr0B+fXo",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ae0d5c9e3234ee38a5ed7aac8f70154": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13c4526f091943648a222b40e3b86386": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f3fd2a74284d33982b9efa64054d5e",
              "IPY_MODEL_b0a28d93c21b4d50b4f8b1594725effe",
              "IPY_MODEL_ccf3dc42d13846268101e0e9786dd5f9"
            ],
            "layout": "IPY_MODEL_d822ed667b0c43479745404d14a693a5"
          }
        },
        "2a300978f2e0499fa4808306bd6d60ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f6e16236a974b6485f30c8c0e4d8e39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6724ac8434e8458d8d41155a8dd03501": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f3fd2a74284d33982b9efa64054d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a300978f2e0499fa4808306bd6d60ba",
            "placeholder": "​",
            "style": "IPY_MODEL_f0be69cd3f944d0781a756f69cbc9427",
            "value": "100%"
          }
        },
        "6f4afca7f59f45dd95612aab08be6315": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0a28d93c21b4d50b4f8b1594725effe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f6e16236a974b6485f30c8c0e4d8e39",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ae0d5c9e3234ee38a5ed7aac8f70154",
            "value": 1
          }
        },
        "ccf3dc42d13846268101e0e9786dd5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6724ac8434e8458d8d41155a8dd03501",
            "placeholder": "​",
            "style": "IPY_MODEL_6f4afca7f59f45dd95612aab08be6315",
            "value": " 1/1 [00:00&lt;00:00, 12.69it/s]"
          }
        },
        "d822ed667b0c43479745404d14a693a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0be69cd3f944d0781a756f69cbc9427": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
